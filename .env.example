# Zen MCP Server Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# API Configuration (Required)
# =============================================================================

# OpenAI-Compatible API Configuration
# Choose ONE of the following options:

# Option 1: Official OpenAI API
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=your_openai_api_key_here

# Option 2: Custom OpenAI-compatible endpoint (Your current setup)
# Your custom API endpoint configuration
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_BASE_URL=https://api-key.info/v1

# Option 3: Local models (Ollama, vLLM, LM Studio, etc.)
# OPENAI_API_KEY=dummy-key  # Some local servers need any non-empty key
# OPENAI_BASE_URL=http://localhost:11434/v1  # Example for Ollama

# =============================================================================
# Optional Configuration
# =============================================================================

# Default model to use when not specified
# Options: 'auto' (Claude picks best model), 'pro', 'flash', 'o3', 'o3-pro', 
#          'o3-mini', 'o4-mini', 'gpt-4.1', 'gpt-4o', 'deepseek-r1'
# When set to 'auto', Claude will select the best model for each task
# Defaults to 'auto' if not specified
DEFAULT_MODEL=pro

# Default thinking mode for ThinkDeep tool
# NOTE: Only applies to models that support extended thinking (e.g., deepseek-r1)
# Token consumption per mode:
#   minimal: 128 tokens   - Quick analysis, fastest response
#   low:     2,048 tokens - Light reasoning tasks  
#   medium:  8,192 tokens - Balanced reasoning (good for most cases)
#   high:    16,384 tokens - Complex analysis (recommended for thinkdeep)
#   max:     32,768 tokens - Maximum reasoning depth, slowest but most thorough
# Defaults to 'high' if not specified
DEFAULT_THINKING_MODE_THINKDEEP=high

# Model usage restrictions (Optional)
# Limit which models can be used for cost control, compliance, or standardization
# Format: Comma-separated list of allowed model names (case-insensitive)
# Empty or unset = all models allowed (default behavior)
#
# Available models:
#   OpenAI Official: o3, o3-mini, o3-pro, o4-mini, gpt-4.1
#   Custom Models: gemini-2.5-pro, gemini-2.5-flash, deepseek-r1, gpt-4o
#   Aliases: pro, flash, mini, gpt4, gpt4.1, gpt4o, deepseek, o3pro, o3mini
#
# Examples:
#   OPENAI_ALLOWED_MODELS=o3-mini,o4-mini,mini     # Only allow mini models (cost control)
#   OPENAI_ALLOWED_MODELS=pro,flash                # Only allow Gemini models
#   OPENAI_ALLOWED_MODELS=deepseek-r1              # Only allow thinking model
#   OPENAI_ALLOWED_MODELS=o3,o3-pro                # Only allow O3 models
#
# Note: These restrictions apply even in 'auto' mode - Claude will only pick from allowed models
# OPENAI_ALLOWED_MODELS=

# Conversation settings
CONVERSATION_TIMEOUT_HOURS=3
MAX_CONVERSATION_TURNS=20

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=DEBUG
